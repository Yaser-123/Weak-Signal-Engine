<html>
    <head>
        <meta charset="utf-8">
        
            <script src="lib/bindings/utils.js"></script>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/dist/vis-network.min.css" integrity="sha512-WgxfT5LWjfszlPHXRmBWHkV2eceiWTOBvrKCNbdgDYTHrT2AeLCGbF4sZlZw3UMN3WtL0tGUoIAKsu8mllg/XA==" crossorigin="anonymous" referrerpolicy="no-referrer" />
            <script src="https://cdnjs.cloudflare.com/ajax/libs/vis-network/9.1.2/dist/vis-network.min.js" integrity="sha512-LnvoEWDFrqGHlHmDD2101OrLcbsfkrzoSpvtSQtxK3RMnRV0eOkhhBN2dXHKRrUU8p2DGRTk35n4O8nWSVe1mQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>
            
        
<center>
<h1></h1>
</center>

<!-- <link rel="stylesheet" href="../node_modules/vis/dist/vis.min.css" type="text/css" />
<script type="text/javascript" src="../node_modules/vis/dist/vis.js"> </script>-->
        <link
          href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/css/bootstrap.min.css"
          rel="stylesheet"
          integrity="sha384-eOJMYsd53ii+scO/bJGFsiCZc+5NDVN2yr8+0RDqr0Ql0h+rP48ckxlpbzKgwra6"
          crossorigin="anonymous"
        />
        <script
          src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.0-beta3/dist/js/bootstrap.bundle.min.js"
          integrity="sha384-JEW9xMcG8R+pH31jmWH6WWP0WintQrMb4s7ZOdauHnUtxwoG2vI5DkLtS3qm9Ekf"
          crossorigin="anonymous"
        ></script>


        <center>
          <h1></h1>
        </center>
        <style type="text/css">

             #mynetwork {
                 width: 100%;
                 height: 600px;
                 background-color: #0e1117;
                 border: 1px solid lightgray;
                 position: relative;
                 float: left;
             }

             

             

             
        </style>
    </head>


    <body>
        <div class="card" style="width: 100%">
            
            
            <div id="mynetwork" class="card-body"></div>
        </div>

        
        

        <script type="text/javascript">

              // initialize global variables.
              var edges;
              var nodes;
              var allNodes;
              var allEdges;
              var nodeColors;
              var originalNodes;
              var network;
              var container;
              var options, data;
              var filter = {
                  item : '',
                  property : '',
                  value : []
              };

              

              

              // This method is responsible for drawing the graph, returns the drawn network
              function drawGraph() {
                  var container = document.getElementById('mynetwork');

                  

                  // parsing and collecting nodes and edges from the python
                  nodes = new vis.DataSet([{"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/product-news/inside-nibi-how-immersion-cooling-and-ai-scale-compute-are-changing-canadian-hpc/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/product-news/inside-nibi-how-immersion-cooling-and-ai-scale-compute-are-changing-canadian-hpc/", "physics": true, "shape": "dot", "size": 6, "title": "Sponsored: Inside Nibi: How immersion cooling and AI-scale compute are changing Canadian HPC. \u003cp\u003eNibi serves as a national platform for advanced research, enabling breakthroughs across disciplines while providing a practical reference for sustainable HPC design at scale\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#f4b000", "font": {"color": "white"}, "id": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "label": "Data Centers: Global Expansion Meets Power and People", "physics": true, "shape": "dot", "size": 82, "title": "Data Centers: Global Expansion Meets Power and People (22 signals)"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/analysis/the-biggest-data-center-stories-of-2025/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/analysis/the-biggest-data-center-stories-of-2025/", "physics": true, "shape": "dot", "size": 6, "title": "The biggest data center stories of 2025. \u003cp\u003eHere\u2019s the news highlights of the biggest year in data centers\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/whitepapers/powering-the-future-of-us-data-centers/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/whitepapers/powering-the-future-of-us-data-centers/", "physics": true, "shape": "dot", "size": 6, "title": "Powering the future of US data centers. \u003cp\u003eHow electricity providers can overcome bottlenecks and supercharge growth\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/google-behind-1bn-data-center-in-little-rock-arkansas-report/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/google-behind-1bn-data-center-in-little-rock-arkansas-report/", "physics": true, "shape": "dot", "size": 6, "title": "Google behind $1bn data center in Little Rock, Arkansas - report. \u003cp\u003eDeveloper\u0027s identity has been a mystery for months\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/anger-over-plans-for-data-center-at-truman-brewery-in-londons-brick-lane/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/anger-over-plans-for-data-center-at-truman-brewery-in-londons-brick-lane/", "physics": true, "shape": "dot", "size": 6, "title": "Anger over plans for data center at Truman Brewery in London\u0027s Brick Lane. \u003cp\u003eResidents say proposed data center threatens to intensify environmental harms\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/digital-halo-secures-469m-in-financing-for-philippines-data-center-project/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/digital-halo-secures-469m-in-financing-for-philippines-data-center-project/", "physics": true, "shape": "dot", "size": 6, "title": "Digital Halo secures $46.9m in financing for Philippines data center project. \u003cp\u003eFinancing will be used to develop its MNL1 data center\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/analysis/2025-telco-talking-points-ai-fiber-frenzy-and-an-ma-boom/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/analysis/2025-telco-talking-points-ai-fiber-frenzy-and-an-ma-boom/", "physics": true, "shape": "dot", "size": 6, "title": "2025 telco talking points: AI, fiber frenzy, and an M\u0026A boom. \u003cp\u003eAI is reshaping the telecoms industry, but is it for better or worse?\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/furiosaai-seeking-a-500m-funding-round-ahead-of-anticipated-ipo-report/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/furiosaai-seeking-a-500m-funding-round-ahead-of-anticipated-ipo-report/", "physics": true, "shape": "dot", "size": 6, "title": "FuriosaAI seeking a $500m funding round ahead of anticipated IPO \u2013 report. \u003cp\u003eCompany expected to receive its first shipment of RNGD AI chips later this month\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/first-phase-of-israels-nvidia-b200-powered-national-ai-supercomputer-goes-live/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/first-phase-of-israels-nvidia-b200-powered-national-ai-supercomputer-goes-live/", "physics": true, "shape": "dot", "size": 6, "title": "First phase of Israel\u2019s Nvidia B200-powered national AI supercomputer goes live. \u003cp\u003eAI cloud provider was awarded tender to build and operate the system last year\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/wom-chile-completes-the-first-phase-of-its-5g-rollout/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/wom-chile-completes-the-first-phase-of-its-5g-rollout/", "physics": true, "shape": "dot", "size": 6, "title": "WOM Chile completes the first phase of its 5G rollout. \u003cp\u003eAs the telco strengthens its national fiber-optic cable\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/zayo-europe-teams-with-reintel-to-launch-high-capacity-iberian-network/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/zayo-europe-teams-with-reintel-to-launch-high-capacity-iberian-network/", "physics": true, "shape": "dot", "size": 6, "title": "Zayo Europe teams with Reintel to launch high-capacity Iberian network. \u003cp\u003ePartnership to create a 400G-enabled dark fiber link connecting data hubs across Spain and Portugal\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/digital-realty-set-to-enter-malaysia-will-acquire-15mw-data-center-outside-kuala-lumpur/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/digital-realty-set-to-enter-malaysia-will-acquire-15mw-data-center-outside-kuala-lumpur/", "physics": true, "shape": "dot", "size": 6, "title": "Digital Realty set to enter Malaysia, will acquire 1.5MW data center outside Kuala Lumpur. \u003cp\u003eAcquisition to close in first half of this year\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/malaysian-fintech-instapay-technologies-selects-alibaba-cloud-infrastructure/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/malaysian-fintech-instapay-technologies-selects-alibaba-cloud-infrastructure/", "physics": true, "shape": "dot", "size": 6, "title": "Malaysian fintech Instapay Technologies selects Alibaba Cloud infrastructure. \u003cp\u003eWill use Alibaba\u0027s local cloud infrastructure to support its payment platform\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/aware-super-invests-300m-in-vantage-apac-takes-minority-stake/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/aware-super-invests-300m-in-vantage-apac-takes-minority-stake/", "physics": true, "shape": "dot", "size": 6, "title": "Aware Super invests $300m in Vantage APAC, takes minority stake. \u003cp\u003eThe investment was made alongside DigitalBridge through Skyline JV\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/bevan-slatterys-subco-plots-subsea-cable-linking-us-to-australia/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/bevan-slatterys-subco-plots-subsea-cable-linking-us-to-australia/", "physics": true, "shape": "dot", "size": 6, "title": "Bevan Slattery\u0027s SubCo plots subsea cable linking US to Australia. \u003cp\u003eCompany plans unrepeatered systems across the Pacific\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/private-equity-firm-cerberus-reinvigorates-delayed-singapore-india-gulf-subsea-cable-project/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/private-equity-firm-cerberus-reinvigorates-delayed-singapore-india-gulf-subsea-cable-project/", "physics": true, "shape": "dot", "size": 6, "title": "Private equity firm Cerberus reinvigorates delayed Singapore-India-Gulf subsea cable project. \u003cp\u003eSet to be operational in 2030\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/national-australia-bank-to-exit-knox-data-center-in-melbourne/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/national-australia-bank-to-exit-knox-data-center-in-melbourne/", "physics": true, "shape": "dot", "size": 6, "title": "National Australia Bank to exit Knox data center in Melbourne. \u003cp\u003eAlso poaches rival CBA AI executive\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/liberty-global-in-talks-to-acquire-three-ireland-report/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/liberty-global-in-talks-to-acquire-three-ireland-report/", "physics": true, "shape": "dot", "size": 6, "title": "Liberty Global in talks to acquire Three Ireland - report. \u003cp\u003eCK Hutchison is looking to divest assets in Europe\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/dcd-downloads/dcdconnect-mena-2026-sample-delegate-list/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/dcd-downloads/dcdconnect-mena-2026-sample-delegate-list/", "physics": true, "shape": "dot", "size": 6, "title": "DCD\u003eConnect | MENA 2026 - Sample Delegate List. \u003cp\u003ePreview who you can expect to meet at DCD\u0026gt;Connect | MENA, taking place at a brand new venue, the Grand Hyatt Dubai, on April 20-21\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/onenergy-completes-construction-of-ai-ups-system-at-national-laboratory-of-the-rockies-colorado/", "label": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/onenergy-completes-construction-of-ai-ups-system-at-national-laboratory-of-the-rockies-colorado/", "physics": true, "shape": "dot", "size": 6, "title": "ON.energy completes construction of AI UPS system at National Laboratory of the Rockies, Colorado. \u003cp\u003eAims to validate its UPS system\u003c/p\u003e"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11935v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11935v1", "physics": true, "shape": "dot", "size": 6, "title": "Big Data Workload Profiling for Energy-Aware Cloud Resource Management. arXiv:2601.11935v1 Announce Type: cross \nAbstract: Cloud data centers face increasing pressure to reduce operational energy consumption as big data workloads continue to grow in scale and complexity. This paper presents a workload aware and energy efficient scheduling framework that profiles CPU utilization, memory demand, and storage IO behavior to guide virtual machine placement decisions. By combining historical execution logs with real time telemetry, the proposed system predicts the energy and performance impact of candidate placements and enables adaptive consolidation while preserving service level agreement compliance. The framework is evaluated using representative Hadoop MapReduce, Spark MLlib, and ETL workloads deployed on a multi node cloud testbed. Experimental results demonstrate consistent energy savings of 15 to 20 percent compared to a baseline scheduler, with negligible performance degradation. These findings highlight workload profiling as a practical and scalable strategy for improving the sustainability of cloud based big data processing environments."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11589v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11589v1", "physics": true, "shape": "dot", "size": 6, "title": "PLA-Serve: A Prefill-Length-Aware LLM Serving System. arXiv:2601.11589v1 Announce Type: cross \nAbstract: PLA-Serve identifies and disaggregates requests with different prompt lengths in LLM serving to reduce TTFT latency. While recent systems have decoupled the prefill and decode stages to improve throughput, they still rely on unified scheduling policies that fail to adapt to heterogeneous workload characteristics. We observe that prompt-length variations lead to distinct performance bottlenecks, motivating an adaptive scheduling strategy. PLA-Serve disaggregates multi-turn long-prefill requests from short-prefill ones and introduces a length-aware smart batching mechanism for short-prefill workloads. It adopts a dual-queue design that supports temporal disaggregation on a single prefill instance or spatial disaggregation across multiple instances. For short-prefill batches, a batch waiting window and CUDA Graph-based clustering mitigate interference from heterogeneous computation, reducing batching delay and lowering average latency. In real multi-turn workloads, PLA-Serve reduces prefill latency by over 30% compared to vanilla SGLang under prefill**--**decode disaggregation, and further decreases SLO violations by 28% in multi-instance deployments with vanilla data-parallel configuration. Compared to the SGLang router with load balancing, it further lowers SLO violations by 12% in multi-GPU settings. Under high concurrency and mixed-request scenarios, PLA-Serve improves request throughput by 35% serving Qwen2.5-32B model for prefill instance, demonstrating its effectiveness in optimizing heterogeneous LLM serving workloads."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11776v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11776v1", "physics": true, "shape": "dot", "size": 6, "title": "Cleansing the Artificial Mind: A Self-Reflective Detoxification Framework for Large Language Models. arXiv:2601.11776v1 Announce Type: cross \nAbstract: Recent breakthroughs in Large Language Models (LLMs) have revealed remarkable generative capabilities and emerging self-regulatory mechanisms, including self-correction and self-rewarding. However, current detoxification techniques rarely exploit these built-in abilities; instead, they rely on external modules, labor-intensive data annotation, or human intervention --factors that hinder scalability and consistency. In this paper, we introduce a fully self-reflective detoxification framework that harnesses the inherent capacities of LLMs to detect, correct toxic content, and refine LLMs without external modules and data annotation. Specifically, we propose a Toxic Signal Detector --an internal self-identification mechanism, coupled with a systematic intervention process to transform toxic text into its non-toxic counterpart. This iterative procedure yields a contrastive detoxification dataset used to fine-tune the model, enhancing its ability for safe and coherent text generation. Experiments on benchmark datasets such as DetoxLLM and ParaDetox show that our method achieves better detoxification performance than state-of-the-art methods while preserving semantic fidelity. By obviating the need for human intervention or external components, this paper reveals the intrinsic self-detoxification ability of LLMs, offering a consistent and effective approach for mitigating harmful content generation. Ultimately, our findings underscore the potential for truly self-regulated language models, paving the way for more responsible and ethically guided text generation systems."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#f4b000", "font": {"color": "white"}, "id": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "label": "AI Enhances Complex Problem Solving in Medicine and Science", "physics": true, "shape": "dot", "size": 132, "title": "AI Enhances Complex Problem Solving in Medicine and Science (666 signals)"}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11778v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11778v1", "physics": true, "shape": "dot", "size": 6, "title": "Translation as a Scalable Proxy for Multilingual Evaluation. arXiv:2601.11778v1 Announce Type: cross \nAbstract: The rapid proliferation of LLMs has created a critical evaluation paradox: while LLMs claim multilingual proficiency, comprehensive non-machine-translated benchmarks exist for fewer than 30 languages, leaving \u003e98% of the world\u0027s 7,000 languages in an empirical void. Traditional benchmark construction faces scaling challenges such as cost, scarcity of domain experts, and data contamination. We evaluate the validity of a simpler alternative: can translation quality alone indicate a model\u0027s broader multilingual capabilities? Through systematic evaluation of 14 models (1B-72B parameters) across 9 diverse benchmarks and 7 translation metrics, we find that translation performance is a good indicator of downstream task success (e.g., Phi-4, median Pearson r: MetricX = 0.89, xCOMET = 0.91, SSA-COMET = 0.87). These results suggest that the representational abilities supporting faithful translation overlap with those required for multilingual understanding. Translation quality, thus emerges as a strong, inexpensive first-pass proxy of multilingual performance, enabling a translation-first screening with targeted follow-up for specific tasks."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11801v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11801v1", "physics": true, "shape": "dot", "size": 6, "title": "RobotDesignGPT: Automated Robot Design Synthesis using Vision Language Models. arXiv:2601.11801v1 Announce Type: cross \nAbstract: Robot design is a nontrivial process that involves careful consideration of multiple criteria, including user specifications, kinematic structures, and visual appearance. Therefore, the design process often relies heavily on domain expertise and significant human effort. The majority of current methods are rule-based, requiring the specification of a grammar or a set of primitive components and modules that can be composed to create a design. We propose a novel automated robot design framework, RobotDesignGPT, that leverages the general knowledge and reasoning capabilities of large pre-trained vision-language models to automate the robot design synthesis process. Our framework synthesizes an initial robot design from a simple user prompt and a reference image. Our novel visual feedback approach allows us to greatly improve the design quality and reduce unnecessary manual feedback. We demonstrate that our framework can design visually appealing and kinematically valid robots inspired by nature, ranging from legged animals to flying creatures. We justify the proposed framework by conducting an ablation study and a user study."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11854v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11854v1", "physics": true, "shape": "dot", "size": 6, "title": "ATOD: An Evaluation Framework and Benchmark for Agentic Task-Oriented Dialogue System. arXiv:2601.11854v1 Announce Type: cross \nAbstract: Recent advances in task-oriented dialogue (TOD) systems, driven by large language models (LLMs) with extensive API and tool integration, have enabled conversational agents to coordinate interleaved goals, maintain long-horizon context, and act proactively through asynchronous execution. These capabilities extend beyond traditional TOD systems, yet existing benchmarks lack systematic support for evaluating such agentic behaviors. To address this gap, we introduce ATOD, a benchmark and synthetic dialogue generation pipeline that produces richly annotated conversations requiring long-term reasoning. ATOD captures key characteristics of advanced TOD, including multi-goal coordination, dependency management, memory, adaptability, and proactivity. Building on ATOD, we propose ATOD-Eval, a holistic evaluation framework that translates these dimensions into fine-grained metrics and supports reproducible offline and online evaluation. We further present a strong agentic memory-based evaluator for benchmarking on ATOD. Experiments show that ATOD-Eval enables comprehensive assessment across task completion, agentic capability, and response quality, and that the proposed evaluator offers a better accuracy-efficiency tradeoff compared to existing memory- and LLM-based approaches under this evaluation setting."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11868v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11868v1", "physics": true, "shape": "dot", "size": 6, "title": "Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces. arXiv:2601.11868v1 Announce Type: cross \nAbstract: AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at https://www.tbench.ai/ ."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12061v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12061v1", "physics": true, "shape": "dot", "size": 6, "title": "Codebook-Injected Dialogue Segmentation for Multi-Utterance Constructs Annotation: LLM-Assisted and Gold-Label-Free Evaluation. arXiv:2601.12061v1 Announce Type: cross \nAbstract: Dialogue Act (DA) annotation typically treats communicative or pedagogical intent as localized to individual utterances or turns. This leads annotators to agree on the underlying action while disagreeing on segment boundaries, reducing apparent reliability. We propose codebook-injected segmentation, which conditions boundary decisions on downstream annotation criteria, and evaluate LLM-based segmenters against standard and retrieval-augmented baselines. To assess these without gold labels, we introduce evaluation metrics for span consistency, distinctiveness, and human-AI distributional agreement. We found DA-awareness produces segments that are internally more consistent than text-only baselines. While LLMs excel at creating construct-consistent spans, coherence-based baselines remain superior at detecting global shifts in dialogue flow. Across two datasets, no single segmenter dominates. Improvements in within-segment coherence frequently trade off against boundary distinctiveness and human-AI distributional agreement. These results highlight segmentation as a consequential design choice that should be optimized for downstream objectives rather than a single performance score."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11859v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11859v1", "physics": true, "shape": "dot", "size": 6, "title": "Cascaded Transformer for Robust and Scalable SLA Decomposition via Amortized Optimization. arXiv:2601.11859v1 Announce Type: cross \nAbstract: The evolution toward 6G networks increasingly relies on network slicing to provide tailored, End-to-End (E2E) logical networks over shared physical infrastructures. A critical challenge is effectively decomposing E2E Service Level Agreements (SLAs) into domain-specific SLAs, which current solutions handle through computationally intensive, iterative optimization processes that incur substantial latency and complexity. To address this, we introduce Casformer, a cascaded Transformer architecture designed for fast, optimization-free SLA decomposition. Casformer leverages historical domain feedback encoded through domain-specific Transformer encoders in its first layer, and integrates cross-domain dependencies using a Transformer-based aggregator in its second layer. The model is trained under a learning paradigm inspired by Domain-Informed Neural Networks (DINNs), incorporating risk-informed modeling and amortized optimization to learn a stable, forward-only SLA decomposition policy. Extensive evaluations demonstrate that Casformer achieves improved SLA decomposition quality against state-of-the-art optimization-based frameworks, while exhibiting enhanced scalability and robustness under volatile and noisy network conditions. In addition, its forward-only design reduces runtime complexity and simplifies deployment and maintenance. These insights reveal the potential of combining amortized optimization with Transformer-based sequence modeling to advance network automation, providing a scalable and efficient solution suitable for real-time SLA management in advanced 5G-and-beyond network environments."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11863v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11863v1", "physics": true, "shape": "dot", "size": 6, "title": "Utilizing Metadata for Better Retrieval-Augmented Generation. arXiv:2601.11863v1 Announce Type: cross \nAbstract: Retrieval-Augmented Generation systems depend on retrieving semantically relevant document chunks to support accurate, grounded outputs from large language models. In structured and repetitive corpora such as regulatory filings, chunk similarity alone often fails to distinguish between documents with overlapping language. Practitioners often flatten metadata into input text as a heuristic, but the impact and trade-offs of this practice remain poorly understood. We present a systematic study of metadata-aware retrieval strategies, comparing plain-text baselines with approaches that embed metadata directly. Our evaluation spans metadata-as-text (prefix and suffix), a dual-encoder unified embedding that fuses metadata and content in a single index, dual-encoder late-fusion retrieval, and metadata-aware query reformulation. Across multiple retrieval metrics and question types, we find that prefixing and unified embeddings consistently outperform plain-text baselines, with the unified at times exceeding prefixing while being easier to maintain. Beyond empirical comparisons, we analyze embedding space, showing that metadata integration improves effectiveness by increasing intra-document cohesion, reducing inter-document confusion, and widening the separation between relevant and irrelevant chunks. Field-level ablations show that structural cues provide strong disambiguating signals. Our code, evaluation framework, and the RAGMATE-10K dataset are publicly hosted."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11895v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11895v1", "physics": true, "shape": "dot", "size": 6, "title": "DevBench: A Realistic, Developer-Informed Benchmark for Code Generation Models. arXiv:2601.11895v1 Announce Type: cross \nAbstract: DevBench is a telemetry-driven benchmark designed to evaluate Large Language Models (LLMs) on realistic code completion tasks. It includes 1,800 evaluation instances across six programming languages and six task categories derived from real developer telemetry, such as API usage and code purpose understanding. Unlike prior benchmarks, it emphasizes ecological validity, avoids training data contamination, and enables detailed diagnostics. The evaluation combines functional correctness, similarity-based metrics, and LLM-judge assessments focused on usefulness and contextual relevance. 9 state-of-the-art models were assessed, revealing differences in syntactic precision, semantic reasoning, and practical utility. Our benchmark provides actionable insights to guide model selection and improvement-detail that is often missing from other benchmarks but is essential for both practical deployment and targeted model development."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11907v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11907v1", "physics": true, "shape": "dot", "size": 6, "title": "Towards Airborne Object Detection: A Deep Learning Analysis. arXiv:2601.11907v1 Announce Type: cross \nAbstract: The rapid proliferation of airborne platforms, including commercial aircraft, drones, and UAVs, has intensified the need for real-time, automated threat assessment systems. Current approaches depend heavily on manual monitoring, resulting in limited scalability and operational inefficiencies. This work introduces a dual-task model based on EfficientNetB4 capable of performing airborne object classification and threat-level prediction simultaneously. To address the scarcity of clean, balanced training data, we constructed the AODTA Dataset by aggregating and refining multiple public sources. We benchmarked our approach on both the AVD Dataset and the newly developed AODTA Dataset and further compared performance against a ResNet-50 baseline, which consistently underperformed EfficientNetB4. Our EfficientNetB4 model achieved 96% accuracy in object classification and 90% accuracy in threat-level prediction, underscoring its promise for applications in surveillance, defense, and airspace management. Although the title references detection, this study focuses specifically on classification and threat-level inference using pre-localized airborne object images provided by existing datasets."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11913v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11913v1", "physics": true, "shape": "dot", "size": 6, "title": "LSTM-MAS: A Long Short-Term Memory Inspired Multi-Agent System for Long-Context Understanding. arXiv:2601.11913v1 Announce Type: cross \nAbstract: Effectively processing long contexts remains a fundamental yet unsolved challenge for large language models (LLMs). Existing single-LLM-based methods primarily reduce the context window or optimize the attention mechanism, but they often encounter additional computational costs or constrained expanded context length. While multi-agent-based frameworks can mitigate these limitations, they remain susceptible to the accumulation of errors and the propagation of hallucinations. In this work, we draw inspiration from the Long Short-Term Memory (LSTM) architecture to design a Multi-Agent System called LSTM-MAS, emulating LSTM\u0027s hierarchical information flow and gated memory mechanisms for long-context understanding. Specifically, LSTM-MAS organizes agents in a chained architecture, where each node comprises a worker agent for segment-level comprehension, a filter agent for redundancy reduction, a judge agent for continuous error detection, and a manager agent for globally regulates information propagation and retention, analogous to LSTM and its input gate, forget gate, constant error carousel unit, and output gate. These novel designs enable controlled information transfer and selective long-term dependency modeling across textual segments, which can effectively avoid error accumulation and hallucination propagation. We conducted an extensive evaluation of our method. Compared with the previous best multi-agent approach, CoA, our model achieves improvements of 40.93%, 43.70%,121.57% and 33.12%, on NarrativeQA, Qasper, HotpotQA, and MuSiQue, respectively."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11920v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11920v1", "physics": true, "shape": "dot", "size": 6, "title": "Enhancing LLM-Based Data Annotation with Error Decomposition. arXiv:2601.11920v1 Announce Type: cross \nAbstract: Large language models offer a scalable alternative to human coding for data annotation tasks, enabling the scale-up of research across data-intensive domains. While LLMs are already achieving near-human accuracy on objective annotation tasks, their performance on subjective annotation tasks, such as those involving psychological constructs, is less consistent and more prone to errors. Standard evaluation practices typically collapse all annotation errors into a single alignment metric, but this simplified approach may obscure different kinds of errors that affect final analytical conclusions in different ways. Here, we propose a diagnostic evaluation paradigm that incorporates a human-in-the-loop step to separate task-inherent ambiguity from model-driven inaccuracies and assess annotation quality in terms of their potential downstream impacts. We refine this paradigm on ordinal annotation tasks, which are common in subjective annotation. The refined paradigm includes: (1) a diagnostic taxonomy that categorizes LLM annotation errors along two dimensions: source (model-specific vs. task-inherent) and type (boundary ambiguity vs. conceptual misidentification); (2) a lightweight human annotation test to estimate task-inherent ambiguity from LLM annotations; and (3) a computational method to decompose observed LLM annotation errors following our taxonomy. We validate this paradigm on four educational annotation tasks, demonstrating both its conceptual validity and practical utility. Theoretically, our work provides empirical evidence for why excessively high alignment is unrealistic in specific annotation tasks and why single alignment metrics inadequately reflect the quality of LLM annotations. In practice, our paradigm can be a low-cost diagnostic tool that assesses the suitability of a given task for LLM annotation and provides actionable insights for further technical optimization."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11956v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11956v1", "physics": true, "shape": "dot", "size": 6, "title": "Double-Calibration: Towards Trustworthy LLMs via Calibrating Knowledge and Reasoning Confidence. arXiv:2601.11956v1 Announce Type: cross \nAbstract: Trustworthy reasoning in Large Language Models (LLMs) is challenged by their propensity for hallucination. While augmenting LLMs with Knowledge Graphs (KGs) improves factual accuracy, existing KG-augmented methods fail to quantify epistemic uncertainty in both the retrieved evidence and LLMs\u0027 reasoning. To bridge this gap, we introduce DoublyCal, a framework built on a novel double-calibration principle. DoublyCal employs a lightweight proxy model to first generate KG evidence alongside a calibrated evidence confidence. This calibrated supporting evidence then guides a black-box LLM, yielding final predictions that are not only more accurate but also well-calibrated, with confidence scores traceable to the uncertainty of the supporting evidence. Experiments on knowledge-intensive benchmarks show that DoublyCal significantly improves both the accuracy and confidence calibration of black-box LLMs with low token cost."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11960v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11960v1", "physics": true, "shape": "dot", "size": 6, "title": "R$^2$PO: Decoupling Training Trajectories from Inference Responses for LLM Reasoning. arXiv:2601.11960v1 Announce Type: cross \nAbstract: Reinforcement learning has become a central paradigm for improving LLM reasoning. However, existing methods use a single policy to produce both inference responses and training optimization trajectories. The objective conflict between generating stable inference responses and diverse training trajectories leads to insufficient exploration, which harms reasoning capability. In this paper, to address the problem, we propose R$^2$PO (Residual Rollout Policy Optimization), which introduces a lightweight Residual Rollout-Head atop the policy to decouple training trajectories from inference responses, enabling controlled trajectory diversification during training while keeping inference generation stable. Experiments across multiple benchmarks show that our method consistently outperforms baselines, achieving average accuracy gains of 3.1% on MATH-500 and 2.4% on APPS, while also reducing formatting errors and mitigating length bias for stable optimization. Our code is publicly available at https://github.com/RRPO-ARR/Code."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11969v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11969v1", "physics": true, "shape": "dot", "size": 6, "title": "$\\texttt{MemoryRewardBench}$: Benchmarking Reward Models for Long-Term Memory Management in Large Language Models. arXiv:2601.11969v1 Announce Type: cross \nAbstract: Existing works increasingly adopt memory-centric mechanisms to process long contexts in a segment manner, and effective memory management is one of the key capabilities that enables large language models to effectively propagate information across the entire sequence. Therefore, leveraging reward models (RMs) to automatically and reliably evaluate memory quality is critical. In this work, we introduce $\\texttt{MemoryRewardBench}$, the first benchmark to systematically study the ability of RMs to evaluate long-term memory management processes. $\\texttt{MemoryRewardBench}$ covers both long-context comprehension and long-form generation tasks, featuring 10 distinct settings with different memory management patterns, with context length ranging from 8K to 128K tokens. Evaluations on 13 cutting-edge RMs indicate a diminishing performance gap between open-source and proprietary models, with newer-generation models consistently outperforming their predecessors regardless of parameter count. We further expose the capabilities and fundamental limitations of current RMs in evaluating LLM memory management across diverse settings."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11977v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11977v1", "physics": true, "shape": "dot", "size": 6, "title": "One-Shot Price Forecasting with Covariate-Guided Experts under Privacy Constraints. arXiv:2601.11977v1 Announce Type: cross \nAbstract: Forecasting in power systems often involves multivariate time series with complex dependencies and strict privacy constraints across regions. Traditional forecasting methods require significant expert knowledge and struggle to generalize across diverse deployment scenarios. Recent advancements in pre-trained time series models offer new opportunities, but their zero-shot performance on domain-specific tasks remains limited. To address these challenges, we propose a novel MoE Encoder module that augments pretrained forecasting models by injecting a sparse mixture-of-experts layer between tokenization and encoding. This design enables two key capabilities: (1) trans forming multivariate forecasting into an expert-guided univariate task, allowing the model to effectively capture inter-variable relations, and (2) supporting localized training and lightweight parameter sharing in federated settings where raw data cannot be exchanged. Extensive experiments on public multivariate datasets demonstrate that MoE-Encoder significantly improves forecasting accuracy compared to strong baselines. We further simulate federated environments and show that transferring only MoE-Encoder parameters allows efficient adaptation to new regions, with minimal performance degradation. Our findings suggest that MoE-Encoder provides a scalable and privacy-aware extension to foundation time series models."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11995v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11995v1", "physics": true, "shape": "dot", "size": 6, "title": "Learning Audio-Visual Embeddings with Inferred Latent Interaction Graphs. arXiv:2601.11995v1 Announce Type: cross \nAbstract: Learning robust audio-visual embeddings requires bringing genuinely related audio and visual signals together while filtering out incidental co-occurrences - background noise, unrelated elements, or unannotated events. Most contrastive and triplet-loss methods use sparse annotated labels per clip and treat any co-occurrence as semantic similarity. For example, a video labeled \"train\" might also contain motorcycle audio and visual, because \"motorcycle\" is not the chosen annotation; standard methods treat these co-occurrences as negatives to true motorcycle anchors elsewhere, creating false negatives and missing true cross-modal dependencies. We propose a framework that leverages soft-label predictions and inferred latent interactions to address these issues: (1) Audio-Visual Semantic Alignment Loss (AV-SAL) trains a teacher network to produce aligned soft-label distributions across modalities, assigning nonzero probability to co-occurring but unannotated events and enriching the supervision signal. (2) Inferred Latent Interaction Graph (ILI) applies the GRaSP algorithm to teacher soft labels to infer a sparse, directed dependency graph among classes. This graph highlights directional dependencies (e.g., \"Train (visual)\" -\u003e \"Motorcycle (audio)\") that expose likely semantic or conditional relationships between classes; these are interpreted as estimated dependency patterns. (3) Latent Interaction Regularizer (LIR): A student network is trained with both metric loss and a regularizer guided by the ILI graph, pulling together embeddings of dependency-linked but unlabeled pairs in proportion to their soft-label probabilities. Experiments on AVE and VEGAS benchmarks show consistent improvements in mean average precision (mAP), demonstrating that integrating inferred latent interactions into embedding learning enhances robustness and semantic coherence."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12019v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12019v1", "physics": true, "shape": "dot", "size": 6, "title": "Acting Flatterers via LLMs Sycophancy: Combating Clickbait with LLMs Opposing-Stance Reasoning. arXiv:2601.12019v1 Announce Type: cross \nAbstract: The widespread proliferation of online content has intensified concerns about clickbait, deceptive or exaggerated headlines designed to attract attention. While Large Language Models (LLMs) offer a promising avenue for addressing this issue, their effectiveness is often hindered by Sycophancy, a tendency to produce reasoning that matches users\u0027 beliefs over truthful ones, which deviates from instruction-following principles. Rather than treating sycophancy as a flaw to be eliminated, this work proposes a novel approach that initially harnesses this behavior to generate contrastive reasoning from opposing perspectives. Specifically, we design a Self-renewal Opposing-stance Reasoning Generation (SORG) framework that prompts LLMs to produce high-quality agree and disagree reasoning pairs for a given news title without requiring ground-truth labels. To utilize the generated reasoning, we develop a local Opposing Reasoning-based Clickbait Detection (ORCD) model that integrates three BERT encoders to represent the title and its associated reasoning. The model leverages contrastive learning, guided by soft labels derived from LLM-generated credibility scores, to enhance detection robustness. Experimental evaluations on three benchmark datasets demonstrate that our method consistently outperforms LLM prompting, fine-tuned smaller language models, and state-of-the-art clickbait detection baselines."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12042v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12042v1", "physics": true, "shape": "dot", "size": 6, "title": "Less Is More -- Until It Breaks: Security Pitfalls of Vision Token Compression in Large Vision-Language Models. arXiv:2601.12042v1 Announce Type: cross \nAbstract: Visual token compression is widely adopted to improve the inference efficiency of Large Vision-Language Models (LVLMs), enabling their deployment in latency-sensitive and resource-constrained scenarios. However, existing work has mainly focused on efficiency and performance, while the security implications of visual token compression remain largely unexplored. In this work, we first reveal that visual token compression substantially degrades the robustness of LVLMs: models that are robust under uncompressed inference become highly vulnerable once compression is enabled. These vulnerabilities are state-specific; failure modes emerge only in the compressed setting and completely disappear when compression is disabled, making them particularly hidden and difficult to diagnose. By analyzing the key stages of the compression process, we identify instability in token importance ranking as the primary cause of this robustness degradation. Small and imperceptible perturbations can significantly alter token rankings, leading the compression mechanism to mistakenly discard task-critical information and ultimately causing model failure. Motivated by this observation, we propose a Compression-Aware Attack to systematically study and exploit this vulnerability. CAA directly targets the token selection mechanism and induces failures exclusively under compressed inference. We further extend this approach to more realistic black-box settings and introduce Transfer CAA, where neither the target model nor the compression configuration is accessible. We further evaluate potential defenses and find that they provide only limited protection. Extensive experiments across models, datasets, and compression methods show that visual token compression significantly undermines robustness, revealing a previously overlooked efficiency-security trade-off."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12049v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12049v1", "physics": true, "shape": "dot", "size": 6, "title": "\\textit{FocaLogic}: Logic-Based Interpretation of Visual Model Decisions. arXiv:2601.12049v1 Announce Type: cross \nAbstract: Interpretability of modern visual models is crucial, particularly in high-stakes applications. However, existing interpretability methods typically suffer from either reliance on white-box model access or insufficient quantitative rigor. To address these limitations, we introduce FocaLogic, a novel model-agnostic framework designed to interpret and quantify visual model decision-making through logic-based representations. FocaLogic identifies minimal interpretable subsets of visual regions-termed visual focuses-that decisively influence model predictions. It translates these visual focuses into precise and compact logical expressions, enabling transparent and structured interpretations. Additionally, we propose a suite of quantitative metrics, including focus precision, recall, and divergence, to objectively evaluate model behavior across diverse scenarios. Empirical analyses demonstrate FocaLogic\u0027s capability to uncover critical insights such as training-induced concentration, increasing focus accuracy through generalization, and anomalous focuses under biases and adversarial attacks. Overall, FocaLogic provides a systematic, scalable, and quantitative solution for interpreting visual models."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12053v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12053v1", "physics": true, "shape": "dot", "size": 6, "title": "A New Strategy for Artificial Intelligence: Training Foundation Models Directly on Human Brain Data. arXiv:2601.12053v1 Announce Type: cross \nAbstract: While foundation models have achieved remarkable results across a diversity of domains, they still rely on human-generated data, such as text, as a fundamental source of knowledge. However, this data is ultimately the product of human brains, the filtered projection of a deeper neural complexity. In this paper, we explore a new strategy for artificial intelligence: moving beyond surface-level statistical regularities by training foundation models directly on human brain data. We hypothesize that neuroimaging data could open a window into elements of human cognition that are not accessible through observable actions, and argue that this additional knowledge could be used, alongside classical training data, to overcome some of the current limitations of foundation models. While previous research has demonstrated the possibility to train classical machine learning or deep learning models on neural patterns, this path remains largely unexplored for high-level cognitive functions. Here, we classify the current limitations of foundation models, as well as the promising brain regions and cognitive processes that could be leveraged to address them, along four levels: perception, valuation, execution, and integration. Then, we propose two methods that could be implemented to prioritize the use of limited neuroimaging data for strategically chosen, high-value steps in foundation model training: reinforcement learning from human brain (RLHB) and chain of thought from human brain (CoTHB). We also discuss the potential implications for agents, artificial general intelligence, and artificial superintelligence, as well as the ethical, social, and technical challenges and opportunities. We argue that brain-trained foundation models could represent a realistic and effective middle ground between continuing to scale current architectures and exploring alternative, neuroscience-inspired solutions."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12055v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12055v1", "physics": true, "shape": "dot", "size": 6, "title": "Automating Parameter Selection in Deep Image Prior for Fluorescence Microscopy Image Denoising via Similarity-Based Parameter Transfer. arXiv:2601.12055v1 Announce Type: cross \nAbstract: Unsupervised deep image prior (DIP) addresses shortcomings of training data requirements and limited generalization associated with supervised deep learning. The performance of DIP depends on the network architecture and the stopping point of its iterative process. Optimizing these parameters for a new image requires time, restricting DIP application in domains where many images need to be processed. Focusing on fluorescence microscopy data, we hypothesize that similar images share comparable optimal parameter configurations for DIP-based denoising, potentially enabling optimization-free DIP for fluorescence microscopy. We generated a calibration (n=110) and validation set (n=55) of semantically different images from an open-source dataset for a network architecture search targeted towards ideal U-net architectures and stopping points. The calibration set represented our transfer basis. The validation set enabled the assessment of which image similarity criterion yields the best results. We then implemented AUTO-DIP, a pipeline for automatic parameter transfer, and compared it to the originally published DIP configuration (baseline) and a state-of-the-art image-specific variational denoising approach. We show that a parameter transfer from the calibration dataset to a test image based on only image metadata similarity (e.g., microscope type, imaged specimen) leads to similar and better performance than a transfer based on quantitative image similarity measures. AUTO-DIP outperforms the baseline DIP (DIP with original DIP parameters) as well as the variational denoising approaches for several open-source test datasets of varying complexity, particularly for very noisy inputs. Applications to locally acquired fluorescence microscopy images further proved superiority of AUTO-DIP."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12068v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12068v1", "physics": true, "shape": "dot", "size": 6, "title": "Bridging the Gap in Bangla Healthcare: Machine Learning Based Disease Prediction Using a Symptoms-Disease Dataset. arXiv:2601.12068v1 Announce Type: cross \nAbstract: Increased access to reliable health information is essential for non-English-speaking populations, yet resources in Bangla for disease prediction remain limited. This study addresses this gap by developing a comprehensive Bangla symptoms-disease dataset containing 758 unique symptom-disease relationships spanning 85 diseases. To ensure transparency and reproducibility, we also make our dataset publicly available. The dataset enables the prediction of diseases based on Bangla symptom inputs, supporting healthcare accessibility for Bengali-speaking populations. Using this dataset, we evaluated multiple machine learning models to predict diseases based on symptoms provided in Bangla and analyzed their performance on our dataset. Both soft and hard voting ensemble approaches combining top-performing models achieved 98\\% accuracy, demonstrating superior robustness and generalization. Our work establishes a foundational resource for disease prediction in Bangla, paving the way for future advancements in localized health informatics and diagnostic tools. This contribution aims to enhance equitable access to health information for Bangla-speaking communities, particularly for early disease detection and healthcare interventions."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12082v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12082v1", "physics": true, "shape": "dot", "size": 6, "title": "Conditional Random Fields for Interactive Refinement of Histopathological Predictions. arXiv:2601.12082v1 Announce Type: cross \nAbstract: Assisting pathologists in the analysis of histopathological images has high clinical value, as it supports cancer detection and staging. In this context, histology foundation models have recently emerged. Among them, Vision-Language Models (VLMs) provide strong yet imperfect zero-shot predictions. We propose to refine these predictions by adapting Conditional Random Fields (CRFs) to histopathological applications, requiring no additional model training. We present HistoCRF, a CRF-based framework, with a novel definition of the pairwise potential that promotes label diversity and leverages expert annotations. We consider three experiments: without annotations, with expert annotations, and with iterative human-in-the-loop annotations that progressively correct misclassified patches. Experiments on five patch-level classification datasets covering different organs and diseases demonstrate average accuracy gains of 16.0% without annotations and 27.5% with only 100 annotations, compared to zero-shot predictions. Moreover, integrating a human in the loop reaches a further gain of 32.6% with the same number of annotations. The code will be made available on https://github.com/tgodelaine/HistoCRF."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#8ecae6", "font": {"color": "white"}, "id": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12099v1", "label": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12099v1", "physics": true, "shape": "dot", "size": 6, "title": "Large language models struggle with ethnographic text annotation. arXiv:2601.12099v1 Announce Type: cross \nAbstract: Large language models (LLMs) have shown promise for automated text annotation, raising hopes that they might accelerate cross-cultural research by extracting structured data from ethnographic texts. We evaluated 7 state-of-the-art LLMs on their ability to annotate 121 ritual features across 567 ethnographic excerpts. Performance was limited, falling well below levels required for reliable automated annotation. Longer texts, features requiring ordinal distinctions, and ambiguous constructs proved particularly difficult. Human inter-coder reliability set an approximate ceiling on LLM accuracy: features that human coders found difficult to agree upon were also difficult for LLMs. Yet even on features where humans reliably agreed, models fell short of human performance. Our findings suggest that LLMs cannot yet substitute for human expertise in ethnographic annotation."}, {"borderWidth": 0, "borderWidthSelected": 0, "color": "#ff9500", "font": {"color": "white"}, "id": "collapsed_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "label": "+641 more", "physics": true, "shape": "box", "size": 12, "title": "This cluster has 641 more signals not shown in the graph.\nSelect the cluster below to view all 666 signals."}]);
                  edges = new vis.DataSet([{"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/product-news/inside-nibi-how-immersion-cooling-and-ai-scale-compute-are-changing-canadian-hpc/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/analysis/the-biggest-data-center-stories-of-2025/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/whitepapers/powering-the-future-of-us-data-centers/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/google-behind-1bn-data-center-in-little-rock-arkansas-report/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/anger-over-plans-for-data-center-at-truman-brewery-in-londons-brick-lane/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/digital-halo-secures-469m-in-financing-for-philippines-data-center-project/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/analysis/2025-telco-talking-points-ai-fiber-frenzy-and-an-ma-boom/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/furiosaai-seeking-a-500m-funding-round-ahead-of-anticipated-ipo-report/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/first-phase-of-israels-nvidia-b200-powered-national-ai-supercomputer-goes-live/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/wom-chile-completes-the-first-phase-of-its-5g-rollout/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/zayo-europe-teams-with-reintel-to-launch-high-capacity-iberian-network/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/digital-realty-set-to-enter-malaysia-will-acquire-15mw-data-center-outside-kuala-lumpur/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/malaysian-fintech-instapay-technologies-selects-alibaba-cloud-infrastructure/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/aware-super-invests-300m-in-vantage-apac-takes-minority-stake/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/bevan-slatterys-subco-plots-subsea-cable-linking-us-to-australia/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/private-equity-firm-cerberus-reinvigorates-delayed-singapore-india-gulf-subsea-cable-project/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/national-australia-bank-to-exit-knox-data-center-in-melbourne/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/liberty-global-in-talks-to-acquire-three-ireland-report/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/dcd-downloads/dcdconnect-mena-2026-sample-delegate-list/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://www.datacenterdynamics.com/rss/::https://www.datacenterdynamics.com/en/news/onenergy-completes-construction-of-ai-ups-system-at-national-laboratory-of-the-rockies-colorado/", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11935v1", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 1.0)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11589v1", "smooth": false, "to": "cluster_80e99598-04cd-43cf-b7e2-a30e7a388b7b", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11776v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11778v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11801v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(14, 17, 23, 0.3)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11854v1", "smooth": false, "to": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11868v1", "value": 0.6618175676895719, "width": 1}, {"color": "rgba(14, 17, 23, 0.3)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11854v1", "smooth": false, "to": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12061v1", "value": 0.661506637455132, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11854v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11859v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11863v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11868v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11895v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11907v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11913v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11920v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11956v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11960v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11969v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11977v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.11995v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12019v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12042v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12049v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12053v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12055v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12061v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12068v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12082v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "rgba(244, 176, 0, 0.4)", "from": "https://rss.arxiv.org/rss/cs.AI::oai:arXiv.org:2601.12099v1", "smooth": false, "to": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}, {"color": "#ff9500", "dashes": true, "from": "cluster_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "smooth": false, "to": "collapsed_53887f68-8186-4fe1-8bf8-781fd8eaf86d", "value": 0.9, "width": 1}]);

                  nodeColors = {};
                  allNodes = nodes.get({ returnType: "Object" });
                  for (nodeId in allNodes) {
                    nodeColors[nodeId] = allNodes[nodeId].color;
                  }
                  allEdges = edges.get({ returnType: "Object" });
                  // adding nodes and edges to the graph
                  data = {nodes: nodes, edges: edges};

                  var options = {"nodes": {"borderWidth": 0, "borderWidthSelected": 0, "chosen": {"node": false, "label": false}, "font": {"strokeWidth": 0}}, "interaction": {"hover": true, "hoverConnectedEdges": false, "selectConnectedEdges": false, "dragNodes": true, "dragView": true, "zoomView": true}, "physics": {"enabled": true, "stabilization": {"enabled": true, "iterations": 200, "updateInterval": 25, "fit": true}, "barnesHut": {"gravitationalConstant": -8000, "centralGravity": 0.3, "springLength": 150, "springConstant": 0.04, "damping": 0.09, "avoidOverlap": 0.5}, "solver": "barnesHut"}};

                  


                  

                  network = new vis.Network(container, data, options);

                  

                  

                  


                  

                  return network;

              }
              drawGraph();
        </script>
    
    <script type="text/javascript">
        var selectedNode = null;
        var physicsDisabled = false;
        
        // Disable physics after stabilization so nodes stay where moved
        network.once("stabilizationIterationsDone", function() {
            network.setOptions({ physics: { enabled: false } });
            physicsDisabled = true;
        });
        
        // When dragging a cluster node, move all connected signal nodes with it
        var dragStartPositions = {};
        var clusterStartPosition = {};
        var isDraggingCluster = false;
        
        network.on("dragStart", function(params) {
            if (params.nodes.length > 0) {
                var nodeId = params.nodes[0];
                
                // Check if this is a cluster node
                if (nodeId.startsWith('cluster_')) {
                    isDraggingCluster = true;
                    // Store initial position of cluster
                    clusterStartPosition = network.getPositions([nodeId])[nodeId];
                    
                    // Get all connected signal nodes
                    var connectedNodes = network.getConnectedNodes(nodeId);
                    dragStartPositions = {};
                    
                    connectedNodes.forEach(function(connectedId) {
                        if (!connectedId.startsWith('cluster_')) {
                            dragStartPositions[connectedId] = network.getPositions([connectedId])[connectedId];
                        }
                    });
                } else {
                    // Dragging a signal node - allow independent movement
                    isDraggingCluster = false;
                }
            }
        });
        
        network.on("dragging", function(params) {
            if (params.nodes.length > 0) {
                var nodeId = params.nodes[0];
                
                // If dragging a cluster, move connected signals
                if (isDraggingCluster && nodeId.startsWith('cluster_') && Object.keys(dragStartPositions).length > 0) {
                    var currentPos = network.getPositions([nodeId])[nodeId];
                    var dx = currentPos.x - clusterStartPosition.x;
                    var dy = currentPos.y - clusterStartPosition.y;
                    
                    // Move all connected signal nodes by the same offset
                    for (var connectedId in dragStartPositions) {
                        var oldPos = dragStartPositions[connectedId];
                        var newX = oldPos.x + dx;
                        var newY = oldPos.y + dy;
                        network.moveNode(connectedId, newX, newY);
                    }
                }
                // If dragging a signal node, it moves independently (default behavior)
            }
        });
        
        network.on("dragEnd", function(params) {
            // Clear drag state
            dragStartPositions = {};
            clusterStartPosition = {};
            isDraggingCluster = false;
        });
        
        network.on("click", function(params) {
            if (params.nodes.length > 0) {
                var nodeId = params.nodes[0];
                var node = network.body.data.nodes.get(nodeId);
                
                // Create persistent popup
                var popup = document.createElement('div');
                popup.id = 'node-popup';
                popup.style.position = 'fixed';
                popup.style.background = '#1e1e1e';
                popup.style.color = 'white';
                popup.style.padding = '15px';
                popup.style.borderRadius = '8px';
                popup.style.maxWidth = '400px';
                popup.style.border = '2px solid #f4b000';
                popup.style.zIndex = '10000';
                popup.style.top = '50%';
                popup.style.left = '50%';
                popup.style.transform = 'translate(-50%, -50%)';
                popup.style.boxShadow = '0 4px 20px rgba(0,0,0,0.5)';
                popup.innerHTML = '<strong>' + (node.label || 'Signal') + '</strong><br><br>' + 
                                 (node.title || 'No content') + 
                                 '<br><br><small style="color: #888;">Click anywhere to close</small>';
                
                // Remove existing popup if any
                var existing = document.getElementById('node-popup');
                if (existing) existing.remove();
                
                document.body.appendChild(popup);
                selectedNode = nodeId;
            } else {
                // Click on empty space - remove popup
                var popup = document.getElementById('node-popup');
                if (popup) popup.remove();
                selectedNode = null;
            }
        });
    </script>
    </body>
</html>